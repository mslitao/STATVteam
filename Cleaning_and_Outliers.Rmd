# Data Cleaning

```{r}
train = read.csv("train.csv")
train_dummy = train[, unlist(lapply(train, is.factor))]
train_dummy$SalePrice = train$SalePrice
```

## Remove predictors with too many NAN
Remove predictors having more than 40% of Nan.
```{r}
num_nan = rep(0, ncol(train_dummy))
for (i in 1:ncol(train_dummy)) {
  num_nan[i] = sum(is.na(train_dummy[,i]))
}
rm_col_idx = which(num_nan/nrow(train_dummy) > 0.4)
colnames(train_dummy)[rm_col_idx]
num_nan[rm_col_idx]/nrow(train_dummy)
```
Above are 5 dummy predictors that should be removed due to too many Nan.
That make sence because not too many houses have above features.

## Remove rows containing NAN
```{r}
train_dummy = train_dummy[,-rm_col_idx]
train_dummy = train_dummy[!rowSums(is.na(train_dummy)),]
```
122 observations containing Nan are removed. There are still 38 dummy predictors, and we want to remove most of them.

## Remove predictors that have too many levels
```{r}
unlist(lapply(lapply(train_dummy, levels), length))
```

If a particular level in a factor occupies too much ratio, like 80%, or a particular level in a factor occupies to little, like 2%, we may think the factor predictor is useless.
```{r}
# function of finding the max amount of level of one factor column
extreme_level_amount = function(vec) {
  max_amount = 0
  min_amount = length(vec)
  for (lv in levels(vec)) {
    max_amount = max(max_amount, sum(vec==lv))
    min_amount = min(min_amount, sum(vec==lv))
  }
  c(max_amount, min_amount)
}
# find the amount of max-level of each factor
max_level_amount_list = rep(0, length(colnames(train_dummy))-1)
min_level_amount_list = rep(0, length(colnames(train_dummy))-1)
for (i in 1:length(colnames(train_dummy))-1) {
  max_level_amount_list[i] = extreme_level_amount(train_dummy[,i])[1]
  min_level_amount_list[i] = extreme_level_amount(train_dummy[,i])[2]
}
rm_col_idx2 = which(max_level_amount_list/nrow(train_dummy)>0.8)
rm_col_idx3 = which(min_level_amount_list/nrow(train_dummy)<0.02)
```

```{r}
rm_col_idx23 = sort(unique(c(rm_col_idx2, rm_col_idx3)))
```


```{r}
library("knitr")
col_ratio_table = data.frame(col_name = colnames(train_dummy)[rm_col_idx2],
                             max_level_ratio = max_level_amount_list[rm_col_idx2]/nrow(train_dummy))
#colnames(train_dummy)[rm_col_idx2]
#max_level_amount_list[rm_col_idx2]/nrow(train_dummy)
kable(col_ratio_table)
#train_dummy = train_dummy[,-rm_col_idx2]
```

```{r}
dummy_hist = function(dummy_col, price_col, amount_factor=100) {
  level_list = levels(dummy_col)
  price_list = rep(0, length(level_list))
  amount_list = rep(0, length(level_list))
  for (i in 1:length(level_list)) {
    lv = level_list[i]
    row_idx = which(dummy_col==lv)
    price_list[i] = mean(price_col[row_idx])
    amount_list[i] = length(row_idx)
  }
  barplot(rbind(price_list, amount_list*amount_factor), beside = TRUE, names.arg=level_list)
}
dummy_hist(train_dummy$BsmtQual, train_dummy$SalePrice)
dummy_hist(train_dummy$GarageFinish, train_dummy$SalePrice)
dummy_hist(train_dummy$ExterQual, train_dummy$SalePrice)
dummy_hist(train_dummy$SaleCondition, train_dummy$SalePrice)
```
```{r}
dummy_hist(train_dummy$HeatingQC, train_dummy$SalePrice)
dummy_hist(train_dummy$CentralAir, train_dummy$SalePrice)
dummy_hist(train_dummy$KitchenQual, train_dummy$SalePrice)
```

## Correlation
```{r}
library(corrplot)
correlations = cor(numeric_train, use = "everything")
```
```{r}
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank")
```

