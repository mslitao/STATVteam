---
title: "STAT 420 Final Project Proposal"
date: "July 17, 2019"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---
***

# Group Members
- Tao Li (taol4)
- Chun Yue Shek (cyshek2)
- Wanlin Yang (wanliny2)

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

# Introduction

**Predicting House Prices With Linear Regression**

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this dataset in our project proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

We will use 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, we will try all the techniques learned in this lession to predict the final price of each home.

***

## Dataset

###  About the dataset

We will use the [Real Estate Data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) dataset for our final project.

- train.csv - the training set
- test.csv - the test set
- data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here

### Understand the dataset

This data is the real estate information with publicly available.Features include LotArea (Lot size in square feet), # of Bedroom, and # of Kitchen There are also categorical features, such as GarageType and MSSubClass(Building class), Utilities.

Here's a brief version of what you'll find in the data description file.

- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.
- MSSubClass: The building class
- MSZoning: The general zoning classification
- LotFrontage: Linear feet of street connected to property
- LotArea: Lot size in square feet
- Street: Type of road access
- Alley: Type of alley access
- LotShape: General shape of property
- LandContour: Flatness of the property
- Utilities: Type of utilities available
- LotConfig: Lot configuration
- LandSlope: Slope of property
- Neighborhood: Physical locations within Ames city limits
- Condition1: Proximity to main road or railroad
- Condition2: Proximity to main road or railroad (if a second is present)
- BldgType: Type of dwelling
- HouseStyle: Style of dwelling
- OverallQual: Overall material and finish quality
- OverallCond: Overall condition rating
- YearBuilt: Original construction date
- YearRemodAdd: Remodel date
- RoofStyle: Type of roof
- RoofMatl: Roof material
- Exterior1st: Exterior covering on house
- Exterior2nd: Exterior covering on house (if more than one material)
- MasVnrType: Masonry veneer type
- MasVnrArea: Masonry veneer area in square feet
- ExterQual: Exterior material quality
- ExterCond: Present condition of the material on the exterior
- Foundation: Type of foundation
- BsmtQual: Height of the basement
- BsmtCond: General condition of the basement
- BsmtExposure: Walkout or garden level basement walls
- BsmtFinType1: Quality of basement finished area
- BsmtFinSF1: Type 1 finished square feet
- BsmtFinType2: Quality of second finished area (if present)
- BsmtFinSF2: Type 2 finished square feet
- BsmtUnfSF: Unfinished square feet of basement area
- TotalBsmtSF: Total square feet of basement area
- Heating: Type of heating
- HeatingQC: Heating quality and condition
- CentralAir: Central air conditioning
- Electrical: Electrical system
- 1stFlrSF: First Floor square feet
- 2ndFlrSF: Second floor square feet
- LowQualFinSF: Low quality finished square feet (all floors)
- GrLivArea: Above grade (ground) living area square feet
- BsmtFullBath: Basement full bathrooms
- BsmtHalfBath: Basement half bathrooms
- FullBath: Full bathrooms above grade
- HalfBath: Half baths above grade
- Bedroom: Number of bedrooms above basement level
- Kitchen: Number of kitchens
- KitchenQual: Kitchen quality
- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
- Functional: Home functionality rating
- Fireplaces: Number of fireplaces
- FireplaceQu: Fireplace quality
- GarageType: Garage location
- GarageYrBlt: Year garage was built
- GarageFinish: Interior finish of the garage
- GarageCars: Size of garage in car capacity
- GarageArea: Size of garage in square feet
- GarageQual: Garage quality
- GarageCond: Garage condition
- PavedDrive: Paved driveway
- WoodDeckSF: Wood deck area in square feet
- OpenPorchSF: Open porch area in square feet
- EnclosedPorch: Enclosed porch area in square feet
- 3SsnPorch: Three season porch area in square feet
- ScreenPorch: Screen porch area in square feet
- PoolArea: Pool area in square feet
- PoolQC: Pool quality
- Fence: Fence quality
- MiscFeature: Miscellaneous feature not covered in other categories
- MiscVal: $Value of miscellaneous feature
- MoSold: Month Sold
- YrSold: Year Sold
- SaleType: Type of sale
- SaleCondition: Condition of sale

## Motivation and Goal

### Motivation

This dataset gives us a chance to look into the data on what really influences the value of a house, so our team members are excited to take a look!

It will be applealing to analyze housing data and deliver a good model predicting price given the predictors of housing information. We may apply the model over the houses we have known around us, and analyse whether there can be some potential influence to the house price that cannot be reflected by data.

### Goals
We plan to 

- Assemble the data and explore the coorelation 
- Clean variables, filtering the outliers and perform feature transformation and selections.
- Build the best model levergaing the knowledge in this course..

To evaluate the model, we want to use Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.) The definition of RMSE will stay the same

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

where

- $y_i$ are the actual values of the response for the given data.
- $\hat{y}_i$ are the predicted values using the fitted model and the predictors from the data.

# Methods
## Load the data
```{r}
data = read.csv("train.csv")

#Remove Id from data
rownames(data) = data$Id
data = data[, -which(names(data) %in% c("Id"))]

```

We want to make some value cleaning and fix invalid data values in the data.
```{r}
# remove missing data, which is stored as "?"
data = subset(data, data$GarageType != "?")

#Fix some NAs
data$GarageYrBlt[is.na(data$GarageYrBlt)] = 1
data$MasVnrArea[is.na(data$MasVnrArea)] = 1
data$LotFrontage[is.na(data$LotFrontage)] = 1
```

## Feature Analysis

### Coorelation Analysis

In this project, we have a big chanllange on the massive predictors. In order to select best correlated predictors and reduce the model complexity. We tried to compute the coorelation for each predictors to the response. 

```{r}
#Try correlation only for numeric features
data_numeric = data[, unlist(lapply(data, is.numeric))]

numericFeatures = names(data_numeric)
numericFeatures = numericFeatures[!numericFeatures %in% c("SalePrice")]

n = length(numericFeatures)
correlations_na_na = rep(0.0, n)
correlations_log_na = rep(0.0, n)
correlations_na_log = rep(0.0, n)
correlations_log_log = rep(0.0, n)

for(i in 1:n){
  correlations_na_na[i] = cor(data[,numericFeatures[i]], data[,"SalePrice"])
  correlations_log_na[i] = cor(log(data[,numericFeatures[i]] +1), data[,"SalePrice"])
  correlations_na_log[i] = cor(data[,numericFeatures[i]], log(data[,"SalePrice"]))
  correlations_log_log[i] = cor(log(data[,numericFeatures[i]]+1), log(data[,"SalePrice"]))
}

colFeaturesName = rep("", n-1)
colFeaturesCorr = rep(0.0, n-1)

featureStat = data.frame(features = numericFeatures, 
                   abs_correlations = round(abs(correlations_na_na),2),
                   correlations_na_na = round(correlations_na_na,2),
                   correlations_log_na = round(correlations_log_na,2),
                   correlations_na_log = round(correlations_na_log,2),
                   correlations_log_log = round(correlations_log_log,2)
                   )
featureStat = featureStat[order(-featureStat$abs_correlations),]
library("knitr")
kable(head(featureStat, 20))
```


Based on the predictor correlation analysis. Let's plot the charts to understand how these top predictors correlated to the response. Here we picked **OverallQual**,  **GrLivArea**, **GarageCars**, **TotalBsmtSF**, **X1stFlrSF**, **GarageArea**, **FullBath**, **YearBuilt**

```{r fig.height=6, fig.width=14}
par(mfrow = c(2, 4))
plot(SalePrice ~ OverallQual, data = data, col = "darkorange", pch = 20,main = "SalePrice VS OverallQual")
plot(SalePrice ~ GrLivArea, data = data, col = "darkorange", pch = 20,main = "SalePrice VS GrLivArea")
plot(SalePrice ~ GarageCars, data = data, col = "darkorange", pch = 20,main = "SalePrice VS GarageCars")
plot(SalePrice ~ TotalBsmtSF, data = data, col = "darkorange", pch = 20,main = "SalePrice VS TotalBsmtSF")
plot(SalePrice ~ X1stFlrSF, data = data, col = "darkorange", pch = 20,main = "SalePrice VS X1stFlrSF")
plot(SalePrice ~ GarageArea, data = data, col = "darkorange", pch = 20,main = "SalePrice VS GarageArea")
plot(SalePrice ~ FullBath, data = data, col = "darkorange", pch = 20,main = "SalePrice VS FullBath")
plot(SalePrice ~ YearBuilt, data = data, col = "darkorange", pch = 20,main = "SalePrice VS YearBuilt")
```

```{r}
#Remove outliers that has larger GrLivArea more than 4000
data = data[which(data$GrLivArea < 4000),]
```

### Collinearity Analysis

```{r echo=FALSE}
n = length(numericFeatures)

colFeaturesName = rep("", n-1)

collinearity_mean = rep(0.0, n)

collinearity_f1 = rep("", n)
collinearity_f2 = rep("", n)
collinearity_f3 = rep("", n)
collinearity_c1 = rep(0.0, n)
collinearity_c2 = rep(0.0, n)
collinearity_c3 = rep(0.0, n)
  
for(i in 1:n){
  index = 1
  for(j in 1:n){
    if(i != j){
      colFeaturesCorr[index] = cor(data[,numericFeatures[i]], data[,numericFeatures[j]])
      colFeaturesName[index] = numericFeatures[j]
      
      index = index +1
      colFeatureStat = data.frame(features = colFeaturesName,
                                  correlations = colFeaturesCorr,
                                  abs_correlations = abs(colFeaturesCorr))
      colFeatureStat = colFeatureStat[order(-colFeatureStat$abs_correlations),]
      collinearity_mean[i] = mean(colFeatureStat$abs_correlations)
      
      collinearity_f1[i] = as.character(colFeatureStat$features[1])
      collinearity_f2[i] = as.character(colFeatureStat$features[2])
      collinearity_f3[i] = as.character(colFeatureStat$features[3])
      
      collinearity_c1[i] = colFeatureStat$correlations[1]
      collinearity_c2[i] = colFeatureStat$correlations[2]
      collinearity_c3[i] = colFeatureStat$correlations[3]
    }
  }
}

feature_collinearity = data.frame(features = numericFeatures, 
                   avg_collinearity = round(collinearity_mean,2),
                   
                   top1_predictor = collinearity_f1,
                   top1_collinearity = round(collinearity_c1,2),
                   top2_predictor = collinearity_f2,
                   top2_collinearity = round(collinearity_c2,2),
                   top3_predictor = collinearity_f3,
                   top3_collinearity = round(collinearity_c3,2)
                   )
feature_collinearity = feature_collinearity[order(-feature_collinearity$avg_collinearity),]
library("knitr")
kable(head(feature_collinearity,20))
```
We found some interesting insights. The collinearity predictors from this statistics has valid meaning in the real world.

For example:

- **Year** based predictors has strong collinearity: 

  YearBuilt - GarageYtBlt - YearRemodAdd - OverallQual

- **Area** based predictors has strong collinearity: 

  GrLivArea - TotRmsAbvGrd - X2ndFlrSF - FullBath
  
  TotalBsmtSF - X1stFlrSF - OverallQual - GarageArea
  
- **Garage, First/Second Floor** has similar and collinearity predictors

  GarageCars - GarageArea - GarageYtBlt - OverallQual

  X1stFlrSF - TotalBsmtSF - GrLivArea - GarageArea

  X2ndFlrSF - GrLivArea - HalfBath - FullBath

Leveraging this inforamtion, we can perform better model and feature selection with siginificant domain knowledge learned from the data.

## Dummy Variables

```{r}
data_dummy = data[, unlist(lapply(data, is.factor))]
data_dummy$SalePrice = data$SalePrice
```

### Remove predictors with too many Nan
Remove predictors having more than 40% of Nan.
```{r}
num_nan = rep(0, ncol(data_dummy))
for (i in 1:ncol(data_dummy)) {
  num_nan[i] = sum(is.na(data_dummy[,i]))
}
rm_col_idx = which(num_nan/nrow(data_dummy) > 0.4)
colnames(data_dummy)[rm_col_idx]
num_nan[rm_col_idx]/nrow(data_dummy)
```
Above are 5 dummy predictors that should be removed due to too many Nan.
That make sence because few houses have above features.

```{r}
# remove rows containing Nan
data_dummy = data_dummy[,-rm_col_idx]
data_dummy = data_dummy[!rowSums(is.na(data_dummy)),]
```

### Remove unbalanced dummy variables

If a particular level in a factor occupies too much ratio, like 99%, or a particular level in a factor occupies to little, like 1%, we may think the factor predictor is not helpful.
```{r}
# function of finding the max amount of level of one factor column
extreme_level_amount = function(vec) {
  max_amount = 0
  min_amount = length(vec)
  for (lv in levels(vec)) {
    max_amount = max(max_amount, sum(vec==lv))
    min_amount = min(min_amount, sum(vec==lv))
  }
  c(max_amount, min_amount)
}
# find the amount of max-level of each factor
max_level_amount_list = rep(0, length(colnames(data_dummy))-1)
min_level_amount_list = rep(0, length(colnames(data_dummy))-1)
for (i in 1:length(colnames(data_dummy))-1) {
  max_level_amount_list[i] = extreme_level_amount(data_dummy[,i])[1]
  min_level_amount_list[i] = extreme_level_amount(data_dummy[,i])[2]
}
rm_unbalanced_col_idx1 = which(max_level_amount_list/nrow(data_dummy)>0.99)
rm_unbalanced_col_idx2 = which(min_level_amount_list/nrow(data_dummy)<0.01)
```

We can see how many levels of each remained dummy variable.
```{r}
rm_unbalanced_col_idx = sort(unique(c(rm_unbalanced_col_idx1, rm_unbalanced_col_idx2)))
remained_dummy_col_idx = seq(1:ncol(data_dummy))[-rm_unbalanced_col_idx]
remained_dummy_col_idx = remained_dummy_col_idx[1:length(remained_dummy_col_idx)-1]

unlist(lapply(lapply(data_dummy[remained_dummy_col_idx], levels), length))
```

There are some numerical predictors related to Bsmt, so we don't consider the Bsmt dummy variables. Same reasons for garage.
To select suitable dummy variables, we want the mean price among levels have a lot difference, and observations of each level have enough amount. So we can plot the barplot of mean price and amout of levels in a particular dummy variable.

Based on above analysis, we decide to use 2 dummy variables.

- CentralAir
- KitchenQual

```{r include=FALSE}
dummy_barplot = function(dummy_col, price_col, amount_factor=100) {
  level_list = levels(dummy_col)
  price_list = rep(0, length(level_list))
  amount_list = rep(0, length(level_list))
  for (i in 1:length(level_list)) {
    lv = level_list[i]
    row_idx = which(dummy_col==lv)
    price_list[i] = mean(price_col[row_idx])
    amount_list[i] = length(row_idx)
  }
  barplot(rbind(price_list, amount_list*amount_factor), beside = TRUE, names.arg=level_list, xlab = "Levels", ylab = "Value", col = c("gray20", "gray80"))
  legend("top", c("Mean Price", "Amount x100"), lwd=15, col = c("gray20", "gray80"))
}
```

```{r fig.height=6, fig.width=14}
par(mfrow = c(1, 2))
dummy_barplot(data_dummy$CentralAir, data_dummy$SalePrice)
dummy_barplot(data_dummy$KitchenQual, data_dummy$SalePrice)
```


## Multiple linear regression

### Define metrics calculation
```{r}
# Calculate RMSE
rmse = function(y, y_hat){
  n = length(y)
  sqrt(sum((y - y_hat) ^ 2) / n)
}

alpha = 0.05

library(lmtest)
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value <= alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

evaluate_model = function(fit, data, response_log = FALSE, round_num = 4){
  predicts = predict(fit, newdata = data)
  predicts[predicts<1] = 1
  
  r2 = summary(fit)$r.squared
  if(response_log){
    rmse_log = rmse(log(data$SalePrice), predicts)
    rmse = rmse(data$SalePrice, exp(predicts))
  }
  else{
    rmse_log = rmse(log(data$SalePrice), log(predicts))
    rmse = rmse(data$SalePrice, predicts)
  }
  c(round(rmse, round_num), round(rmse_log, round_num), round(get_loocv_rmse(fit), round_num), round(r2, round_num), round(get_adj_r2(fit), round_num), 
    get_bp_decision(fit, alpha), round(bptest(fit)$p.value, round_num),
    get_sw_decision(fit, alpha), round(shapiro.test(resid(fit))$p.value, round_num),
    get_num_params(fit))
}

evaluate_model_name = c("RMSE", "RMSE Logged", "loocv_RMSE", "R2", "Adj. R2", "bp_decision", "bp_p_value", "sw_decision", "sw_p_value", "num_params")
```

### Training and Testing Data
```{r}
set.seed(20190801)

selected_columns = c(as.character(subset(featureStat, featureStat$abs_correlations>0.05)$features), "SalePrice", "CentralAir", "KitchenQual")


data_selected = data[, which(names(data) %in% selected_columns)]


sample = sample.int(n = nrow(data_selected), size = floor(.7*nrow(data_selected)), replace = F)

training = data_selected[sample,]
testing  = data_selected[-sample,]
```

### Full model with all numeric predictors

```{r}
fit = lm(SalePrice ~ ., data = training)

# RMSE on train and test data, log and raw
df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit,training),
                Test = evaluate_model(fit,testing))

kable(df)

# AIC
fit_aic = step(fit,trace = 0)
df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit_aic,training),
                Test = evaluate_model(fit_aic,testing))

kable(df)
```

## Tranformations

### Response Tranformations 

Based on the analysis, we want to apply log transformation to response field.

```{r}
fit = lm(log(SalePrice) ~ ., data = training)
#summary(model1)

# RMSE on train and test data, log and raw
df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit, training, TRUE),
                Test = evaluate_model(fit, testing, TRUE))

kable(df)
```

We can see that with transofrmation to the response, there's a significant improvement.

### Predictor Tranformations

- Feature range is larger (Area), we picked the LotArea, GrLivArea, X1stFlrSF, GarageArea

```{r}
fit = lm(log(SalePrice) ~ . + log(LotArea) + log(GrLivArea)+log(X1stFlrSF)+log(GarageArea), data = training)

df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit, training, TRUE),
                Test = evaluate_model(fit, testing, TRUE))

kable(df)
```

We can see slightly improvemtn by applying tranformation to predictors. <TODO> add more analysis here.

## Interaction

For top important features

- OverallQual
- GrLivArea
- GarageCars
- GarageCars
- YearBuilt
- YearRemodAdd

```{r}
fit = lm(log(SalePrice) ~ . + (OverallQual + GrLivArea + GarageCars + TotalBsmtSF + X1stFlrSF  + MasVnrArea + Fireplaces+ YearBuilt ) ^2 +YearBuilt:TotalBsmtSF:X1stFlrSF + YearBuilt:MasVnrArea:GarageCars + log(LotArea)  + log(GrLivArea)+log(X1stFlrSF)+log(GarageArea) , data = training)

df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit, training, TRUE),
                Test = evaluate_model(fit, testing, TRUE))

kable(df)
```


## Model selection

```{r}
# AIC
fit_aic = step(fit,trace = 0)


df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit_aic, training, TRUE),
                Test = evaluate_model(fit_aic, testing, TRUE))

kable(df)
```


## Outlier diagnostics

```{r}

fit = lm(formula = log(SalePrice) ~ LotFrontage + OverallQual + OverallCond + 
    YearBuilt  + BsmtFinSF1 + TotalBsmtSF + 
    log(X2ndFlrSF +1) + BsmtFullBath + FullBath + HalfBath + 
    KitchenAbvGr + Fireplaces + GarageCars  + WoodDeckSF + 
    OpenPorchSF + ScreenPorch + CentralAir + KitchenQual + log(LotArea) + 
    log(GrLivArea):YearBuilt + log(GarageArea) + I(X1stFlrSF^2)+
    OverallQual:MasVnrArea + X2ndFlrSF:Fireplaces:YearBuilt, 
    data = training)


df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit, training, TRUE),
                Test = evaluate_model(fit, testing, TRUE))
df

cd_fit = cooks.distance(fit)
large_cd_fit = cd_fit > 4 / length(cd_fit)

fit = lm(formula = log(SalePrice) ~ LotFrontage + OverallQual + OverallCond + 
    YearBuilt  + BsmtFinSF1 + TotalBsmtSF + 
    log(X2ndFlrSF +1) + BsmtFullBath + FullBath + HalfBath + 
    KitchenAbvGr + Fireplaces + GarageCars  + WoodDeckSF + 
    OpenPorchSF + ScreenPorch + CentralAir + KitchenQual + log(LotArea) + 
    log(GrLivArea):YearBuilt + log(GarageArea) + I(X1stFlrSF^2)+
    OverallQual:MasVnrArea + X2ndFlrSF:Fireplaces:YearBuilt, 
    data = training,
    subset = cd_fit <= 4 / length(cd_fit)
    )


df = data.frame(Measure = evaluate_model_name,
                Train = evaluate_model(fit, training, TRUE),
                Test = evaluate_model(fit, testing, TRUE))

kable(df)
```

```{r}
#single table comparsion of the model 
df = data.frame(Measure = evaluate_model_name,
                fit_Train = evaluate_model(fit, training, TRUE),
                fit_Test = evaluate_model(fit, testing, TRUE),
                fit_aic_Train = evaluate_model(fit_aic, training, TRUE),
                fit_aic_Test = evaluate_model(fit_aic, testing, TRUE)
                )

kable(df)
```

```{r}
fit_final = fit_aic
```

- Based the above the above table, the best model is selected and stored as `fit_final`



## Variance Inflation Factor (VIF) 

```{r fig.width=14}
library(faraway)
head(sort(vif(fit_final), decreasing = TRUE), 40)
```

- `r names(vif(fit_final)[which.max(vif(fit_final))])` has the largest VIF
- Most of predictors have very high VIF. They suggest multicollinearity. It does not affect prediction. However, we should aware the multicollinearity exist in our best model for predication  

## Residual diagnostics

```{r}
plot_fitted_resid(fit_final)
```
```{r}
plot_qq(fit_final)
```
```{r}
df = data.frame(Measure = evaluate_model_name,
                fit_final_Train = evaluate_model(fit, training, TRUE),
                fit_final_Test = evaluate_model(fit, testing, TRUE))

kable(df)
```

### Other techniques

# Results

# Discussion
In this project, we want to find a suitable model to predict the price of a house given the properties of it. But the original data has 79 predictors, which seems too much for a linear model. So we should do data cleaning at first. We firstly removed predictors that have too many `Nan`, meaning the data may not be available for most houses. Then we split the predictors into factor predictors and numerical predictors.

In numerical predictors, we mainly focused on the colinearity of each predictor with each other. For the predictors that highly colinear related to others, we picked the representative one in our model. For example, `GrageCars` is colinear related to `GarageCars`, `GarageArea`, `GarageYtBlt` etc. This relationship intuitively makes sense. Based on the approach of analyzing data and its practical meaning behind that, we found 5 important features of a house. Our later transformations and interactions are mainly based on the top important predictors.

At the same time, we also investigated the dummy variables. We hope to find a set of dummy variables to convey some information that numerical variables don't have. A dummy variable should have a balanced amount of observations among different levels, and the house price of each level should have an obvious difference. So we selected `CentralAir` and `KitchenQual` as the dummy variable in our model.

Moreover, we did a lot of tuning to make the model to satisfy the equal-variance assumption and normality assumption.

## Parameters of Final Model
The expression of our final model is
$$
log(y) = \beta_0 + \beta_1x_{LotFrontage} + \beta_2x_{OverallQual} + \beta_3x_{OverallCond} + \beta_4x_{YearBuilt} + \beta_5x_{BsmtFinSF1} + \beta_6x_{TotalBsmtSF} \\
+ \beta_7log(x_{X2ndFlrSF}+1) + \beta_8x_{BsmtFullBath} + \beta_9x_{FullBath} + \beta_{10}x_{HalfBath} + \beta_{11}x_{KitchenAbvGr} + \beta_{12}x_{Fireplaces} \\
+ \beta_{13}x_{GarageCars} + \beta_{14}x_{WoodDeckSF} + \beta_{15}x_{OpenPorchSF} + \beta_{16}x_{ScreenPorch} + \beta_{17}x_{CentralAirY} + \beta_{18}x_{KitchenQualFa} \\
+ \beta_{19}x_{KitchenQualGd} + \beta_{20}x_{KitchenQualTA} + \beta_{21}log(x_{LotArea}) + \beta_{22}log(x_{GarageArea}) + \beta_{23}x_{X1stFlrSF}^2 + \beta_{24}x_{YearBuilt}log(x_{GrLivArea}) \\
+ \beta_{25}x_{OverallQual}x_{MasVnrArea} + \beta_{26}x_{YearBuilt}x_{Fireplaces}x_{X2ndFlrSF}
$$

There are totally 27 parameters.

## A prediction example
We can pick an observation from test set.

# Appendix

